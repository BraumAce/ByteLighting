import{_ as l,c as p,a as e,g as i,b as n,d as r,o}from"./app-JLiA5lxj.js";const a={};function g(s,t){return o(),p("div",null,[t[0]||(t[0]=e("h1",{id:"写法-测评-——-调试-agent-的-prompt",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#写法-测评-——-调试-agent-的-prompt"},[e("span",null,"“写法 + 测评” —— 调试 Agent 的 Prompt")])],-1)),t[1]||(t[1]=e("ul",null,[e("li",null,[i("相关资料："),e("a",{href:"https://www.youtube.com/watch?v=XSZP9GhhuAc",target:"_blank",rel:"noopener noreferrer"},"Prompting for Agents | Code w/ Claude")])],-1)),n(" more "),t[2]||(t[2]=r('<h2 id="什么是-agent" tabindex="-1"><a class="header-anchor" href="#什么是-agent"><span>什么是 Agent？</span></a></h2><p>Agent = 在一个循环回路中使用工具的模型。</p><p>给定目标后，它会自主地：<strong>选择/调用合适工具 → 观察反馈 → 更新决策 → 继续推进，直到满足停止条件</strong>。</p><p>它所依赖的 “环境” 包括：运行环境、可用工具清单、提示词（任务与边界）。</p><p>Agent 的组成：</p><ul><li>目标与停止条件；</li><li>工具回路（Action–Observation）：每一步都基于工具返回更新计划；</li><li>环境三件套：运行环境 / 工具 / 系统提示（明确 “Agent 要完成什么”）；</li><li>“越简单越好”：提示与工具说明要清晰克制，别给花里胡哨的语言描述；</li></ul><h2 id="什么是-提示词工程-prompt-engineering" tabindex="-1"><a class="header-anchor" href="#什么是-提示词工程-prompt-engineering"><span>什么是 “提示词工程（prompt engineering）” ？</span></a></h2><p>Anthropic AI 团队成员 Hannah 认为 “提示词工程是一种系统性改进用于大模型应用的提示词的方法 —— 通过测试、评估、分析与对提示词及工具的优化，用自然语言进行编程！</p><p>一个好的提示词工程是所需能力包含：</p><ul><li>清晰、无歧义、精确的写作；</li><li>以科学思维制作评测，不断测试；</li><li>产品化思维：对你的产品来说，理想的模型行为是什么；</li><li>理解大模型的倾向与局限；</li><li>汇总并分析失败模式，并思考修复方法；</li><li>思考边界场景，让提示在广泛输入下都稳健。</li></ul><p><strong>写提示词 = 写操作流程</strong></p><p>Anthropic 的核心观点：Prompt 不是文案，是规则和流程。它要解决的不是 “说得漂亮”，而是如何让 Agent 在真实环境里把事办完。</p><p>这份规程至少包含五部分：</p><ul><li>角色与高层目标（1–2 句话说清你是谁、为啥而来）</li><li>动态上下文（检索到的资料、用户偏好、会话历史）</li><li>详细任务指令（做什么、不做什么、成败标准）</li><li>示例 n-shot（可选，用于边界提醒，不是“写死流程”）</li><li>重复关键指令（长提示里尤其重要，防遗忘）</li></ul><p>一个示例：</p><blockquote><p>你是一名 AI 旅行顾问（AI travel agent），任务是根据用户输入创建一份个性化旅行行程。<br> 你的目标是：产出一份有吸引力、结构清晰、切实可行的行程，既符合用户偏好，也匹配指定的目的地与出行天数。</p><p>你将获得以下信息：目的地、天数、用户偏好。</p><p>在制定行程时，请遵循以下指南：</p><ul><li>调研目的地及其热门景点，并结合用户偏好。</li><li>为每一天规划活动，确保观光—放松—本地体验之间的良好平衡。</li><li>给出用餐建议，考虑当地美食；如用户提到饮食偏好，请一并考虑。</li><li>推荐住宿选项，需与用户偏好与预算相匹配。</li><li>提供实用信息，如交通方式与预估花费。</li><li>注意可用天数，制定现实可行的时间安排。</li></ul><p>请按以下格式呈现你的行程：</p><ul><li>以目的地的简要介绍开头。</li><li>提供按天拆分的活动、用餐与住宿安排。</li><li>以额外提示/出行建议结尾。</li></ul><p>现在，请基于提供的目的地、天数和用户偏好，创建一份个性化旅行行程。你的建议应当既有创意又充分详尽，确保行程既体现用户兴趣，也凸显目的地的独特之处。</p></blockquote><h2 id="什么时候该用-agent" tabindex="-1"><a class="header-anchor" href="#什么时候该用-agent"><span>什么时候该用 Agent？</span></a></h2><p>不是所有场景都需要 Agent（很多时候反而不该用 Agent），它最适合复杂且高价值的任务。人可以按 “固定步骤” 一步步做完的工作，用工作流（workflow）可能更合适、更省资源。</p><p><strong>判断方法</strong>:</p><ol><li><p>任务是否复杂到只知道终点，不清楚具体怎么走、需要哪些信息与工具；</p></li><li><p>完成后是否 “高价值” ？低价值流程别浪费 Agent 的资源；</p></li><li><p>工具与数据是否可用？若给不了足够工具/信息就让它办成事，那就先缩小范围；</p></li><li><p>容错与纠错成本：难以发现/纠正的错误，不适合让 Agent 全自动去跑。</p></li></ol><p><strong>2 个适合 Agent 的例子</strong>：</p><ol><li><p>写代码：你知道目标是 “把设计文档落实成 PR”，但并不确定具体路径、要如何迭代修改 —— 高价值、强杠杆，适合 Agent。</p></li><li><p>数据分析：你清楚希望得到哪些洞见/可视化，但数据形态、清洗过程并不确定 —— 这类探索式任务非常适配 Agent。</p></li></ol><h2 id="如何写好-agent-的-prompt" tabindex="-1"><a class="header-anchor" href="#如何写好-agent-的-prompt"><span>如何写好 Agent 的 Prompt？</span></a></h2><p><strong>原则一：像 Agent 那样思考</strong></p><p>模拟 “Agent 身处的环境” —— 它能看到什么工具、工具会返回什么；</p><p>甚至可以在脑内模拟一遍：如果你站在 Agent 的角度，拿到这份工具说明，你会不会困惑？人类都看不懂的工作流，模型更不可能做对。</p><p><strong>原则二：赋予 “合理的启发式（Heuristics）”</strong></p><p>提示工程不是 “写字”，而是决定模型该拥有哪些概念与行为准则。</p><p>例如，我们给 Agent 一个重要概念 —— “不可逆性（irreversibility）”：避免做不可逆、可能伤害用户或环境的动作。</p><p>再比如，给 “检索” Agent 设停止条件与预算：找到答案就停；简单问题 &lt; 5 次工具调用，复杂问题可到 10 或 50；</p><p>不要为 “完美来源” 无限搜索。这类 “合理启发式” 都要在提示里清晰明确地写出来。</p><p><strong>原则三：明确 “何时用哪种工具”</strong></p><p>前沿模型一次能挂非常多的工具，但模型并不知道在你的组织里哪个工具对哪个任务更关键。</p><p>必须在提示中写出选择原则：比如公司信息优先搜 Slack、代码问题查 GitHub/Sentry、业务报表走 DataDog…… 别只给一串简短描述就指望它自己悟透。</p><p><strong>原则四：引导 “思考 — 行动” 的过程</strong></p><p>不要只 “打开思考”，而要具体引导：</p><ul><li><p>让它先规划：这个查询复杂度如何？预期用几次工具？优先查哪些来源？如何判定成功？</p></li><li><p>在工具调用之间穿插反思：网页结果不必然正确，需要质量评估/二次求证/必要的免责声明。</p></li><li><p>提前写上副作用与停止条件：比如“若找不到完美来源，最多 N 次搜索后停止”。</p></li></ul><p><strong>原则五：管理上下文窗口</strong></p><p>Agent 容易撞到上下文上限。做法包括：</p><ul><li><p>压缩：临近窗口上限时自动把上下文浓缩为高密度摘要，交给新的会话继续跑。</p></li><li><p>外部记忆：把关键过程/状态写入外部文件，需要时再读取。</p></li><li><p>子 Agent：把搜索等 “吃上下文” 的工作分给子代理，压缩后再交给主代理整合、撰写报告。</p></li></ul><p><strong>原则六：让 Claude 发挥所长 + 工具要 “少而精”</strong></p><p>先用一套最小可用的提示和工具跑起来，再逐步加复杂度。避免给一堆名字相似/职责重叠的工具（例如 6 个 “搜索” 工具查不同库），会让模型混淆 —— 能合并就合并，并把用途说清清楚楚。</p><h2 id="测评怎么做-从-小而真-开始" tabindex="-1"><a class="header-anchor" href="#测评怎么做-从-小而真-开始"><span>测评怎么做：从 “小而真” 开始</span></a></h2><ul><li><p>效果量越大，样本越可小：起步不需要上百条，只要几条真实用例，每次改 Prompt / 工具文档都能看到显著变化。</p></li><li><p>用真实任务评测：尽量让评测题就像用户会问的那种，且能用现有工具找到标准答案。</p></li><li><p>LLM 评审 + 量表（rubric） 很有用：只要规则清楚，模型能胜任 “打分官”。</p></li><li><p>人评无法被完全替代：每周都要有人 “猛怼 + 手感校验 + 真实用户试用”，人类最能摸到 “硌手的边角”。</p></li></ul><h2 id="评测都评什么-结果-过程" tabindex="-1"><a class="header-anchor" href="#评测都评什么-结果-过程"><span>评测都评什么：结果 &amp; 过程</span></a></h2><ol><li><strong>结果向（Outcome）</strong></li></ol><ul><li><p>答案正确性：用 LLM-judge 判定回答是否正确、是否覆盖关键点。</p></li><li><p>最终状态达成：看 Agent 是否到达正确的最终状态（例如：外部系统里确实发生了期望变更）。</p></li></ul><ol start="2"><li><strong>过程向（Process）</strong></li></ol><ul><li><p>工具使用正确性：评估是否选对工具与参数，以及遇错能否恢复（图示中“从参数错误恢复”的示例）。</p></li><li><p>其他常见过程量化：步骤数/时延、无效调用、异常与回退等 —— 这些直接从对话与调用日志即可统计。</p></li></ul><p>LLM-as-judge 的最小做法：给评审模型一份量表和结构化输出格式，它就能稳定工作。</p><p><strong>示例量表要点</strong></p><ul><li>是否满足硬性约束（0/1/2）</li><li>证据质量与可追溯性（0/1/2）</li><li>取舍与理由是否清晰（0/1/2）</li><li>是否给出风险/不确定性（0/1/2）</li><li>输出契约是否遵守（0/1/2）</li><li>合计 0–10 分，并给一句话短评</li></ul><p><strong>测评起步流程</strong></p><ul><li><p>选 10–20 条真实任务样例（最好能用现有工具找到明确答案/标准）。</p></li><li><p>为每条样例写明期望结果/最终状态（方便做 τ-bench）。</p></li><li><p>准备一份 rubric，用 LLM-as-judge 打分；必要处穿插人评抽检。</p></li><li><p>观察结果 + 过程两套指标的变化；对失败样例做回放，定位是选择错工具、参数错误、步骤冗长还是停止条件/启发式不当。</p></li><li><p>小改就复测：每次只调整一个维度（如 Prompt 的启发式或某个工具文档），再跑同一小集合对比效果。</p></li></ul>',54))])}const m=l(a,[["render",g],["__file","“写法_测评” — 调试Agent的Prompt.html.vue"]]),u=JSON.parse('{"path":"/computer/llm/application/%E2%80%9C%E5%86%99%E6%B3%95_%E6%B5%8B%E8%AF%84%E2%80%9D%20%E2%80%94%20%E8%B0%83%E8%AF%95Agent%E7%9A%84Prompt.html","title":"“写法 + 测评” —— 调试 Agent 的 Prompt","lang":"zh-CN","frontmatter":{"category":["LLM 应用"],"tag":["Prompt","Agent"],"description":"相关资料：Prompting for Agents | Code w/ Claude","head":[["meta",{"property":"og:url","content":"https://blog.bytelighting.cn/computer/llm/application/%E2%80%9C%E5%86%99%E6%B3%95_%E6%B5%8B%E8%AF%84%E2%80%9D%20%E2%80%94%20%E8%B0%83%E8%AF%95Agent%E7%9A%84Prompt.html"}],["meta",{"property":"og:site_name","content":"ByteLighting"}],["meta",{"property":"og:title","content":"“写法 + 测评” —— 调试 Agent 的 Prompt"}],["meta",{"property":"og:description","content":"相关资料：Prompting for Agents | Code w/ Claude"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-09-21T17:33:19.000Z"}],["meta",{"property":"article:tag","content":"Prompt"}],["meta",{"property":"article:tag","content":"Agent"}],["meta",{"property":"article:modified_time","content":"2025-09-21T17:33:19.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"“写法 + 测评” —— 调试 Agent 的 Prompt\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-21T17:33:19.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BraumAce\\",\\"url\\":\\"https://blog.bytelighting.cn/article\\"}]}"]]},"headers":[{"level":2,"title":"什么是 Agent？","slug":"什么是-agent","link":"#什么是-agent","children":[]},{"level":2,"title":"什么是 “提示词工程（prompt engineering）” ？","slug":"什么是-提示词工程-prompt-engineering","link":"#什么是-提示词工程-prompt-engineering","children":[]},{"level":2,"title":"什么时候该用 Agent？","slug":"什么时候该用-agent","link":"#什么时候该用-agent","children":[]},{"level":2,"title":"如何写好 Agent 的 Prompt？","slug":"如何写好-agent-的-prompt","link":"#如何写好-agent-的-prompt","children":[]},{"level":2,"title":"测评怎么做：从 “小而真” 开始","slug":"测评怎么做-从-小而真-开始","link":"#测评怎么做-从-小而真-开始","children":[]},{"level":2,"title":"评测都评什么：结果 & 过程","slug":"评测都评什么-结果-过程","link":"#评测都评什么-结果-过程","children":[]}],"git":{"createdTime":1758465403000,"updatedTime":1758475999000,"contributors":[{"name":"BraumAce","email":"siyuan.peng@foxmail.com","commits":1}]},"readingTime":{"minutes":8.01,"words":2404},"filePathRelative":"computer/llm/application/“写法+测评” — 调试Agent的Prompt.md","localizedDate":"2025年9月21日","excerpt":"\\n<ul>\\n<li>相关资料：<a href=\\"https://www.youtube.com/watch?v=XSZP9GhhuAc\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Prompting for Agents | Code w/ Claude</a></li>\\n</ul>\\n","autoDesc":true}');export{m as comp,u as data};
